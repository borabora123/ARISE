{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":92337,"databundleVersionId":11336108,"sourceType":"competition"},{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889},{"sourceId":11957969,"sourceType":"datasetVersion","datasetId":7518617},{"sourceId":414566,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":338392,"modelId":359342},{"sourceId":414940,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":338618,"modelId":359630}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport torch\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nimport matplotlib.patches as patches\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:10.690970Z","iopub.execute_input":"2025-05-27T23:21:10.691266Z","iopub.status.idle":"2025-05-27T23:21:12.299779Z","shell.execute_reply.started":"2025-05-27T23:21:10.691241Z","shell.execute_reply":"2025-05-27T23:21:12.299012Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport json\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom torchvision.models.detection.image_list import ImageList\nfrom torchvision.ops import MultiScaleRoIAlign\n\n\n\ndef normalize_boxes(boxes, image_width, image_height):\n\n    boxes = boxes.clone()\n    boxes[:, [0, 2]] /= image_width   # x coordinates\n    boxes[:, [1, 3]] /= image_height  # y coordinates\n    return boxes\n    \nclass RADataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None):\nt_dir\n        self.transform = transform\n\n        with open(os.path.join(root_dir, 'train_val_split.json')) as f:\n            splits = json.load(f)\n            \n        self.image_files = splits[split]\n        \n        self.df = pd.read_csv('/kaggle/input/good-df-for-arise/good_data_frame_ARISE.csv')\n        \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, 'jpeg', str(self.image_files[idx]))+'.jpeg'\n        image = Image.open(img_name).convert('RGB')\n        \n        img_width, img_height = image.size\n\n        patient_id = self.image_files[idx]\n        \n        patient_joints = self.df[(self.df['patient_id'] == patient_id)]\n        \n\n        boxes = []\n        jsn_scores = []\n        erosion_scores = []\n        joint_types = []\n        \n        for _, row in patient_joints.iterrows():\n            joint_id = row['joint_id']\n            box = row[['xcenter','ycenter','dx','dy']]\n            if len(box) > 0:\n                \n                xcenter = float(box['xcenter'])\n                ycenter = float(box['ycenter'])\n                width = float(box['dx'])\n                height = float(box['dy'])\n                \n                xmin = xcenter - width\n                ymin = ycenter - height\n                xmax = xcenter + width\n                ymax = ycenter + height\n                \n                \n                boxes.append([xmin, ymin, xmax, ymax])\n                jsn_scores.append(int(row['jsn_score']))\n                erosion_scores.append(int(row['erosion_score']))\n                joint_types.append(joint_id)\n        \n        # Convert to tensors\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        jsn_scores = torch.as_tensor(jsn_scores, dtype=torch.int32)\n        erosion_scores = torch.as_tensor(erosion_scores, dtype=torch.int32)\n        joint_types = torch.as_tensor(joint_types, dtype=torch.int64)\n        \n        target = {\n            'boxes': boxes,\n            'jsn_scores': jsn_scores,\n            'erosion_scores': erosion_scores,\n            'labels': joint_types,\n            'image_id': torch.tensor(patient_id)}\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:12.301769Z","iopub.execute_input":"2025-05-27T23:21:12.301993Z","iopub.status.idle":"2025-05-27T23:21:12.986990Z","shell.execute_reply.started":"2025-05-27T23:21:12.301971Z","shell.execute_reply":"2025-05-27T23:21:12.986322Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"path = '/kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset'\n# Data transforms\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Create datasets\ntrain_dataset = RADataset(root_dir=path, split='train', transform=transform)\nval_dataset = RADataset(root_dir=path, split='val', transform=transform)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:12.988381Z","iopub.execute_input":"2025-05-27T23:21:12.988712Z","iopub.status.idle":"2025-05-27T23:21:13.041162Z","shell.execute_reply.started":"2025-05-27T23:21:12.988678Z","shell.execute_reply":"2025-05-27T23:21:13.040591Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\nval_loader = DataLoader(\n    val_dataset, batch_size=1, shuffle=False, num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:13.042098Z","iopub.execute_input":"2025-05-27T23:21:13.042310Z","iopub.status.idle":"2025-05-27T23:21:13.047510Z","shell.execute_reply.started":"2025-05-27T23:21:13.042290Z","shell.execute_reply":"2025-05-27T23:21:13.046856Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"model = torch.load('/kaggle/input/arise/pytorch/default/1/model.pth', map_location=torch.device('cpu'))\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:13.050852Z","iopub.execute_input":"2025-05-27T23:21:13.051181Z","iopub.status.idle":"2025-05-27T23:21:18.391812Z","shell.execute_reply.started":"2025-05-27T23:21:13.051147Z","shell.execute_reply":"2025-05-27T23:21:18.391094Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d()\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d()\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d()\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d()\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d()\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d()\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d()\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d()\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (layer_blocks): ModuleList(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign()\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=42, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=168, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"torch.save(model, 'model_cpu.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.393817Z","iopub.execute_input":"2025-05-27T23:21:18.394056Z","iopub.status.idle":"2025-05-27T23:21:18.678022Z","shell.execute_reply.started":"2025-05-27T23:21:18.394031Z","shell.execute_reply":"2025-05-27T23:21:18.677366Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# \n# for imgs, annotations in train_loader:\n#     imgs = list(img.to(device) for img in imgs)\n#     annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n#     print(annotations)\n#     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.679062Z","iopub.execute_input":"2025-05-27T23:21:18.679279Z","iopub.status.idle":"2025-05-27T23:21:18.682137Z","shell.execute_reply.started":"2025-05-27T23:21:18.679257Z","shell.execute_reply":"2025-05-27T23:21:18.681430Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"# from torchvision.ops import box_iou\n\n# num_epochs = 40\n# model.to(device)\n    \n\n# #all_losses\n\n# len_dataloader = len(train_loader)\n# iou_val_loss_epoch = []\n# train_loss_epoch = []\n# for epoch in range(num_epochs):\n     \n#     i = 0\n#     epoch_loss = []\n#     iou_train_loss = []\n#     device = 'cuda'\n#     for imgs, annotations in train_loader:\n#         model.train() \n#         imgs = list(img.to(device) for img in imgs)\n#         annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n#         loss_dict = model([imgs[0]], [annotations[0]])\n#         losses = sum(loss for loss in loss_dict.values())\n        \n#         optimizer.zero_grad()\n#         losses.backward()\n#         optimizer.step() \n        \n#         epoch_loss.append(losses.cpu().item())\n        \n#     scheduler.step()\n    \n#     train_loss_epoch.append(np.mean(epoch_loss))\n    \n#     val_loss = []\n#     iou_val_loss = []\n#     model.eval()\n#     for imgs, annotations in val_loader:\n#         imgs = list(img.to(device) for img in imgs)\n#         annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n#         prediction = model([imgs[0]])\n\n                \n#         ious = box_iou(prediction[0]['boxes'], annotations[0]['boxes'][prediction[0]['labels']])\n#         try:\n#             max_ious, _ = torch.max(ious, dim=1)\n#             iou_val_loss.append(max_ious.cpu().mean().item())\n#         except Exception as e:\n#             iou_val_loss.append(0)\n#     iou_val_loss_epoch.append(np.mean(iou_val_loss))\n\n#     if epoch>0 and iou_val_loss_epoch[epoch]>iou_val_loss_epoch[epoch-1]:\n#         torch.save(model.state_dict(), 'best_model_state_dict.pth')\n\n#     print(f\"Epoch: {epoch}, Trainloss: {train_loss_epoch[epoch]} IOU_val: {iou_val_loss_epoch[epoch]}, Learning rate: {scheduler.get_last_lr()[0]:.5f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.683187Z","iopub.execute_input":"2025-05-27T23:21:18.683441Z","iopub.status.idle":"2025-05-27T23:21:18.693615Z","shell.execute_reply.started":"2025-05-27T23:21:18.683415Z","shell.execute_reply":"2025-05-27T23:21:18.692749Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#for imgs, annotations in val_loader:\n#        imgs = list(img.to(device) for img in imgs)\n#        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n#        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.694583Z","iopub.execute_input":"2025-05-27T23:21:18.694799Z","iopub.status.idle":"2025-05-27T23:21:18.707442Z","shell.execute_reply.started":"2025-05-27T23:21:18.694778Z","shell.execute_reply":"2025-05-27T23:21:18.706701Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#model.eval()\n#preds = model(imgs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.708294Z","iopub.execute_input":"2025-05-27T23:21:18.708595Z","iopub.status.idle":"2025-05-27T23:21:18.721003Z","shell.execute_reply.started":"2025-05-27T23:21:18.708568Z","shell.execute_reply":"2025-05-27T23:21:18.720298Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def get_preds(tensor, bboxes):\n    unique_values = torch.unique(tensor[0])\n    if len(unique_values) <= 42:\n        for i in range(42):\n            if i not in unique_values:\n                unique_values = torch.cat([unique_values, torch.tensor([i], dtype=torch.float32)])\n                tensor = torch.cat([tensor, torch.tensor([[i], [0.1]], dtype=torch.float32)], dim=1)\n                bboxes = torch.cat([bboxes,torch.tensor([[0,0,5000,5000]], dtype=torch.float32)], dim=0)\n    selected_indices = []\n    for val in unique_values:\n        # Mask where dim=0 equals current unique value\n        mask = (tensor[0] == val)\n        # Get corresponding scores\n        scores = tensor[1][mask]\n        # Find the index of the max score among these\n        max_idx = torch.argmax(scores).item()\n        # Get the original index in the full tensor\n        original_indices = torch.where(mask)[0]\n        selected_idx = original_indices[max_idx].item()\n        selected_indices.append(selected_idx)\n    \n    # Convert to a tensor\n    selected_indices = torch.tensor(selected_indices, device=tensor.device)\n\n\n    return {'boxes':bboxes[selected_indices]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.721948Z","iopub.execute_input":"2025-05-27T23:21:18.722165Z","iopub.status.idle":"2025-05-27T23:21:18.731208Z","shell.execute_reply.started":"2025-05-27T23:21:18.722144Z","shell.execute_reply":"2025-05-27T23:21:18.730556Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import os\n\neval_paths = os.listdir('/kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/eval_data')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.732129Z","iopub.execute_input":"2025-05-27T23:21:18.732400Z","iopub.status.idle":"2025-05-27T23:21:18.748871Z","shell.execute_reply.started":"2025-05-27T23:21:18.732375Z","shell.execute_reply":"2025-05-27T23:21:18.748208Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import cv2\ndef crop_image_and_save(patient_id, image, bbox, joint_id):\n\n    width = image.shape[1]\n    height = image.shape[2]\n\n    xmin, ymin, xmax, ymax = bbox\n    \n    if xmin>xmax:\n        print(xmin, xmax)\n        xmin,xmax = xmax,xmin\n        print(xmin, xmax)\n    if ymin>ymax:\n        ymin,ymax = ymax,ymin\n        \n    x1 = int(max(xmin, 1))\n    y1 = int(max(ymin, 1))\n    x2 = int(min(xmax, height-1))\n    y2 = int(min(ymax, width-1))\n\n\n        \n        \n    cropped_img = (image[:, y1:y2, x1:x2].permute(1,2,0).numpy()*255).astype(np.int)\n\n    output_path = '/kaggle/working/eval_cropped/'+f\"{patient_id}_{joint_id}.jpeg\"\n    \n    try:\n        cv2.imwrite(output_path, cropped_img)\n    except Exception as e:\n        print(x1, y1, x2, y2)\n        print(image.shape)\n        print(cropped_img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.749827Z","iopub.execute_input":"2025-05-27T23:21:18.750040Z","iopub.status.idle":"2025-05-27T23:21:18.937863Z","shell.execute_reply.started":"2025-05-27T23:21:18.750019Z","shell.execute_reply":"2025-05-27T23:21:18.937262Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Eval_Dataset(Dataset):\n    def __init__(self, eval_pahts, transform=None):\n        self.eval_pahts = eval_pahts\n        self.transform = transform\n        self.root_dir = '/kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset'\n            \n    def __len__(self):\n        return len(self.eval_pahts)\n    \n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, 'eval_data', str(self.eval_pahts[idx]))\n        image = Image.open(img_name).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n\n        return image,  int(self.eval_pahts[idx].split('.')[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.938753Z","iopub.execute_input":"2025-05-27T23:21:18.938980Z","iopub.status.idle":"2025-05-27T23:21:18.944919Z","shell.execute_reply.started":"2025-05-27T23:21:18.938957Z","shell.execute_reply":"2025-05-27T23:21:18.944139Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"eval_dataset = Eval_Dataset(eval_paths, transform)\n\neval_loader = DataLoader(\n    eval_dataset, batch_size=1, shuffle=False, num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.946215Z","iopub.execute_input":"2025-05-27T23:21:18.946588Z","iopub.status.idle":"2025-05-27T23:21:18.955922Z","shell.execute_reply.started":"2025-05-27T23:21:18.946553Z","shell.execute_reply":"2025-05-27T23:21:18.955155Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\neval_df = pd.DataFrame(columns=['patient_id', 'joint_id', 'bbox'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.957019Z","iopub.execute_input":"2025-05-27T23:21:18.957366Z","iopub.status.idle":"2025-05-27T23:21:18.969238Z","shell.execute_reply.started":"2025-05-27T23:21:18.957301Z","shell.execute_reply":"2025-05-27T23:21:18.968655Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from tqdm import tqdm\n\nos.makedirs('/kaggle/working/eval_cropped/', exist_ok=True)\n\nmodel.eval()\nfor img, patient_id in tqdm(eval_loader):\n    imgs = img[0].to(device)\n    predictions = [{k:v.detach().cpu() for k,v in model([imgs])[0].items()}]\n    imgs = imgs.detach().cpu()\n    tensor = torch.stack([predictions[0]['labels'].float(), predictions[0]['scores']])\n    bboxes = get_preds(tensor, predictions[0]['boxes'])\n    for i, bbox in enumerate(bboxes['boxes']):\n        row = {'patient_id':patient_id[0],'joint_id':i,'bbox':bbox}\n        eval_df= eval_df.append(row, ignore_index=True)\n        crop_image_and_save(patient_id[0], imgs, bbox, i)\n\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:18.970133Z","iopub.execute_input":"2025-05-27T23:21:18.970332Z","iopub.status.idle":"2025-05-27T23:21:36.033055Z","shell.execute_reply.started":"2025-05-27T23:21:18.970310Z","shell.execute_reply":"2025-05-27T23:21:36.032085Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/30 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n\tnonzero(Tensor input, *, Tensor out)\nConsider using one of the following signatures instead:\n\tnonzero(Tensor input, *, bool as_tuple)\n100%|██████████| 30/30 [00:17<00:00,  1.76it/s]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"len(eval_paths)*42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:36.034937Z","iopub.execute_input":"2025-05-27T23:21:36.035372Z","iopub.status.idle":"2025-05-27T23:21:36.042685Z","shell.execute_reply.started":"2025-05-27T23:21:36.035285Z","shell.execute_reply":"2025-05-27T23:21:36.041906Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"1260"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"train_df = pd.DataFrame(columns=['patient_id', 'joint_id', 'bbox'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:36.043591Z","iopub.execute_input":"2025-05-27T23:21:36.043797Z","iopub.status.idle":"2025-05-27T23:21:36.056465Z","shell.execute_reply.started":"2025-05-27T23:21:36.043776Z","shell.execute_reply":"2025-05-27T23:21:36.055557Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel.eval()\nfor img, annot in tqdm(train_loader):\n    patient_id = [annot[0]['image_id'].item()]\n    imgs = img[0].to(device)\n    predictions = [{k:v.detach().cpu() for k,v in model([imgs])[0].items()}]\n    imgs = imgs.detach().cpu()\n    tensor = torch.stack([predictions[0]['labels'].float(), predictions[0]['scores']])\n    bboxes = get_preds(tensor, predictions[0]['boxes'])\n    \n    for i, bbox in enumerate(bboxes['boxes']):\n        row = {'patient_id':patient_id[0],'joint_id':i,'bbox':bbox}\n        train_df = train_df.append(row, ignore_index=True)\n        crop_image_and_save(patient_id[0], imgs, bbox, i)\n\n\nfor img, annot in tqdm(val_loader):\n    patient_id = [annot[0]['image_id'].item()]\n    imgs = img[0].to(device)\n    predictions = [{k:v.detach().cpu() for k,v in model([imgs])[0].items()}]\n    imgs = imgs.detach().cpu()\n    tensor = torch.stack([predictions[0]['labels'].float(), predictions[0]['scores']])\n    bboxes = get_preds(tensor, predictions[0]['boxes'])\n    for i, bbox in enumerate(bboxes['boxes']):\n        row = {'patient_id':patient_id[0],'joint_id':i,'bbox':bbox}\n        train_df = train_df.append(row, ignore_index=True)\n        crop_image_and_save(patient_id[0], imgs, bbox, i)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:21:36.057409Z","iopub.execute_input":"2025-05-27T23:21:36.057633Z","iopub.status.idle":"2025-05-27T23:24:42.265034Z","shell.execute_reply.started":"2025-05-27T23:21:36.057611Z","shell.execute_reply":"2025-05-27T23:24:42.264076Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 240/240 [02:27<00:00,  1.62it/s]\n100%|██████████| 60/60 [00:38<00:00,  1.57it/s]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"len(os.listdir('/kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/jpeg'))*42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:24:52.947588Z","iopub.execute_input":"2025-05-27T23:24:52.947895Z","iopub.status.idle":"2025-05-27T23:24:52.954573Z","shell.execute_reply.started":"2025-05-27T23:24:52.947870Z","shell.execute_reply":"2025-05-27T23:24:52.953661Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"12600"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:24:53.587779Z","iopub.execute_input":"2025-05-27T23:24:53.588059Z","iopub.status.idle":"2025-05-27T23:24:53.625919Z","shell.execute_reply.started":"2025-05-27T23:24:53.588035Z","shell.execute_reply":"2025-05-27T23:24:53.625031Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"      patient_id joint_id                                               bbox\n0            165        0  [tensor(1487.9550), tensor(502.1628), tensor(1...\n1            165        1  [tensor(1632.1521), tensor(595.7759), tensor(1...\n2            165        2  [tensor(1851.4111), tensor(848.0338), tensor(2...\n3            165        3  [tensor(1317.0197), tensor(1897.6770), tensor(...\n4            165        4  [tensor(1584.0186), tensor(1765.0103), tensor(...\n...          ...      ...                                                ...\n12595        483       37  [tensor(474.5116), tensor(654.5651), tensor(79...\n12596        483       38  [tensor(303.5309), tensor(631.4346), tensor(63...\n12597        483       39  [tensor(179.4075), tensor(657.3367), tensor(47...\n12598        483       40  [tensor(56.3359), tensor(746.2819), tensor(358...\n12599        483       41  [tensor(0.), tensor(0.), tensor(5000.), tensor...\n\n[12600 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>joint_id</th>\n      <th>bbox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165</td>\n      <td>0</td>\n      <td>[tensor(1487.9550), tensor(502.1628), tensor(1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>165</td>\n      <td>1</td>\n      <td>[tensor(1632.1521), tensor(595.7759), tensor(1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>165</td>\n      <td>2</td>\n      <td>[tensor(1851.4111), tensor(848.0338), tensor(2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>165</td>\n      <td>3</td>\n      <td>[tensor(1317.0197), tensor(1897.6770), tensor(...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>165</td>\n      <td>4</td>\n      <td>[tensor(1584.0186), tensor(1765.0103), tensor(...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12595</th>\n      <td>483</td>\n      <td>37</td>\n      <td>[tensor(474.5116), tensor(654.5651), tensor(79...</td>\n    </tr>\n    <tr>\n      <th>12596</th>\n      <td>483</td>\n      <td>38</td>\n      <td>[tensor(303.5309), tensor(631.4346), tensor(63...</td>\n    </tr>\n    <tr>\n      <th>12597</th>\n      <td>483</td>\n      <td>39</td>\n      <td>[tensor(179.4075), tensor(657.3367), tensor(47...</td>\n    </tr>\n    <tr>\n      <th>12598</th>\n      <td>483</td>\n      <td>40</td>\n      <td>[tensor(56.3359), tensor(746.2819), tensor(358...</td>\n    </tr>\n    <tr>\n      <th>12599</th>\n      <td>483</td>\n      <td>41</td>\n      <td>[tensor(0.), tensor(0.), tensor(5000.), tensor...</td>\n    </tr>\n  </tbody>\n</table>\n<p>12600 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"eval_df.to_csv('eval_df.csv', index=False)\ntrain_df.to_csv('train_df.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:24:55.867331Z","iopub.execute_input":"2025-05-27T23:24:55.867737Z","iopub.status.idle":"2025-05-27T23:24:58.429056Z","shell.execute_reply.started":"2025-05-27T23:24:55.867707Z","shell.execute_reply":"2025-05-27T23:24:58.428268Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"len(os.listdir('/kaggle/working/eval_cropped'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:25:09.895093Z","iopub.execute_input":"2025-05-27T23:25:09.895425Z","iopub.status.idle":"2025-05-27T23:25:09.908625Z","shell.execute_reply.started":"2025-05-27T23:25:09.895393Z","shell.execute_reply":"2025-05-27T23:25:09.907856Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"13860"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"# Function to plot image","metadata":{}},{"cell_type":"code","source":"def plot_image(img_tensor, annotation):\n    \n    fig,ax = plt.subplots(1)\n    img = img_tensor.cpu().data\n\n    # Display the image\n    ax.imshow(img.permute(1, 2, 0))\n    \n    for box in annotation[\"boxes\"]:\n        xmin, ymin, xmax, ymax = box\n\n        # Create a Rectangle patch\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T23:04:26.646189Z","iopub.execute_input":"2025-05-27T23:04:26.646441Z","iopub.status.idle":"2025-05-27T23:04:26.652662Z","shell.execute_reply.started":"2025-05-27T23:04:26.646417Z","shell.execute_reply":"2025-05-27T23:04:26.651886Z"}},"outputs":[],"execution_count":24}]}